# Gemini ì–´ëŒ‘í„° íŒ¨í„´ êµ¬í˜„

## ğŸ“‹ ê°œìš”

OpenAI ì–´ëŒ‘í„°ì™€ ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ Google Gemini API ì–´ëŒ‘í„°ë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

**êµ¬í˜„ ë‚ ì§œ**: 2025-10-13
**ìƒíƒœ**: âœ… ì™„ë£Œ

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### ì–´ëŒ‘í„° ê³„ì¸µ êµ¬ì¡°

```
ModelPort (ì¸í„°í˜ì´ìŠ¤)
    â†‘
BaseGeminiAdapter (ì¶”ìƒ í´ë˜ìŠ¤)
    â†‘
    â”œâ”€ GeminiChatAdapter (ì¼ë°˜ ì±„íŒ…)
    â””â”€ GeminiWebSearchAdapter (ì›¹ ê²€ìƒ‰)
```

### íŒŒì¼ êµ¬ì¡°

```
src/infrastructure/ai_model/adapters/
â”œâ”€â”€ base_openai_adapter.py         # ê¸°ì¡´
â”œâ”€â”€ openai_chat_adapter.py          # ê¸°ì¡´
â”œâ”€â”€ openai_reasoning_adapter.py     # ê¸°ì¡´
â”œâ”€â”€ base_gemini_adapter.py          # ğŸ†• Gemini ë² ì´ìŠ¤
â”œâ”€â”€ gemini_chat_adapter.py          # ğŸ†• Gemini Chat
â””â”€â”€ gemini_web_search_adapter.py    # ğŸ†• Gemini ì›¹ì„œì¹˜
```

---

## ğŸ”§ êµ¬í˜„ ìƒì„¸

### 1. BaseGeminiAdapter

**ìœ„ì¹˜**: `src/infrastructure/ai_model/adapters/base_gemini_adapter.py`

**ì—­í• **:
- Gemini API ê³µí†µ ê¸°ëŠ¥ ì œê³µ
- HTTP í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬
- ìš”ì²­/ì‘ë‹µ ì²˜ë¦¬
- ì—ëŸ¬ í•¸ë“¤ë§

**ì£¼ìš” ê¸°ëŠ¥**:

#### API ì¸ì¦
```python
# OpenAI: Authorization í—¤ë”
headers = {"Authorization": f"Bearer {api_key}"}

# Gemini: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
params = {"key": api_key}
```

#### HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
```python
def _create_http_client(self) -> httpx.Client:
    return httpx.Client(
        base_url=self._base_url,
        headers={"Content-Type": "application/json"},
        params={"key": self._api_key},  # GeminiëŠ” ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
        timeout=self._timeout
    )
```

#### API ìš”ì²­
```python
def _make_request(
    self,
    endpoint: str,
    payload: Dict[str, Any]
) -> Result[Dict[str, Any], str]:
    """ê³µí†µ HTTP ìš”ì²­ ì²˜ë¦¬"""
    try:
        response = self._http_client.post(endpoint, json=payload)
        response.raise_for_status()
        return Success(response.json())
    except httpx.HTTPStatusError as e:
        return Failure(f"Gemini API ì˜¤ë¥˜: {e}")
```

#### ì‘ë‹µ íŒŒì‹±
```python
def _parse_response(
    self,
    response_data: Dict[str, Any]
) -> Result[ModelResponse, str]:
    """
    Gemini ì‘ë‹µ í˜•ì‹:
    {
      "candidates": [{
        "content": {
          "parts": [{"text": "..."}],
          "role": "model"
        }
      }],
      "usageMetadata": {
        "promptTokenCount": 10,
        "candidatesTokenCount": 50
      }
    }
    """
    candidates = response_data["candidates"]
    content = candidates[0]["content"]
    text = "".join(part["text"] for part in content["parts"])

    return Success(ModelResponse.create(
        content=text,
        model="gemini",
        usage=response_data["usageMetadata"]
    ))
```

---

### 2. GeminiChatAdapter

**ìœ„ì¹˜**: `src/infrastructure/ai_model/adapters/gemini_chat_adapter.py`

**ì—­í• **:
- ì¼ë°˜ ì±„íŒ… ëŒ€í™” ì²˜ë¦¬
- OpenAI ë©”ì‹œì§€ í˜•ì‹ â†’ Gemini í˜•ì‹ ë³€í™˜
- Generation Config ì„¤ì •

**ì§€ì› ëª¨ë¸**:
- `gemini-2.0-flash-exp` (ìµœì‹ , ì¶”ì²œ)
- `gemini-1.5-pro`
- `gemini-1.5-flash`

**ì£¼ìš” ë³€í™˜**:

#### ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
```python
# OpenAI í˜•ì‹
{
  "messages": [
    {"role": "system", "content": "You are..."},
    {"role": "user", "content": "Hello"}
  ]
}

# Gemini í˜•ì‹
{
  "contents": [
    {"role": "user", "parts": [{"text": "You are..."}]},
    {"role": "user", "parts": [{"text": "Hello"}]}
  ]
}
```

#### Role ë§¤í•‘
```python
def _map_role(self, openai_role: str) -> str:
    role_mapping = {
        "system": "user",      # system â†’ userë¡œ í†µí•©
        "user": "user",
        "assistant": "model"   # assistant â†’ model
    }
    return role_mapping.get(openai_role, "user")
```

#### Generation Config ë³€í™˜
```python
# OpenAI ì„¤ì •
{
  "temperature": 0.7,
  "max_tokens": 1000,
  "top_p": 0.9
}

# Gemini ì„¤ì •
{
  "generationConfig": {
    "temperature": 0.7,
    "maxOutputTokens": 1000,
    "topP": 0.9
  }
}
```

---

### 3. GeminiWebSearchAdapter

**ìœ„ì¹˜**: `src/infrastructure/ai_model/adapters/gemini_web_search_adapter.py`

**ì—­í• **:
- Google Search Grounding í™œì„±í™”
- ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
- ìµœì‹  ì •ë³´ í™œìš©

**Google Search Grounding**:

#### Grounding ë„êµ¬ ì„¤ì •
```python
def _build_grounding_tool(self) -> Dict[str, Any]:
    return {
        "googleSearchRetrieval": {
            "dynamicRetrievalConfig": {
                "mode": "MODE_DYNAMIC",
                "dynamicThreshold": 0.7
            }
        }
    }
```

#### í˜ì´ë¡œë“œ êµ¬ì„±
```python
{
  "contents": [...],
  "generationConfig": {...},
  "tools": [{
    "googleSearchRetrieval": {
      "dynamicRetrievalConfig": {
        "mode": "MODE_DYNAMIC",
        "dynamicThreshold": 0.7
      }
    }
  }]
}
```

**ë™ì  ì„ê³„ê°’ (dynamicThreshold)**:
- `0.0-1.0` ì‚¬ì´ ê°’
- ë†’ì„ìˆ˜ë¡ ì›¹ ê²€ìƒ‰ ë¹ˆë„ ì¦ê°€
- `0.7` ê¶Œì¥ (ê· í˜•)

---

## ğŸ†š OpenAI vs Gemini ë¹„êµ

| í•­ëª© | OpenAI | Gemini |
|------|--------|--------|
| **ì¸ì¦ ë°©ì‹** | Authorization í—¤ë” | ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° |
| **ë©”ì‹œì§€ í˜•ì‹** | `messages` | `contents` |
| **Role** | system/user/assistant | user/model |
| **ì„¤ì •** | ì§ì ‘ í¬í•¨ | `generationConfig` |
| **ì›¹ ê²€ìƒ‰** | ìë™ (GPT-4o) | `googleSearchRetrieval` ë„êµ¬ |
| **ì‘ë‹µ í˜•ì‹** | `choices` | `candidates` |
| **Token ì •ë³´** | `usage` | `usageMetadata` |

---

## âœ… í…ŒìŠ¤íŠ¸

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í•­ëª©

1. **BaseGeminiAdapter**
   - âœ… API í‚¤ ê²€ì¦
   - âœ… HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
   - âœ… ìš”ì²­ ì„±ê³µ/ì‹¤íŒ¨ ì²˜ë¦¬
   - âœ… ì‘ë‹µ íŒŒì‹±

2. **GeminiChatAdapter**
   - âœ… ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
   - âœ… Role ë§¤í•‘
   - âœ… Generation Config ë³€í™˜
   - âœ… ì—”ë“œí¬ì¸íŠ¸ êµ¬ì„±

3. **GeminiWebSearchAdapter**
   - âœ… Grounding ë„êµ¬ ì¶”ê°€
   - âœ… ë™ì  ì„ê³„ê°’ ì„¤ì •
   - âœ… ì›¹ ê²€ìƒ‰ í™œì„±í™”

---

## ğŸš€ ì‚¬ìš© ì˜ˆì‹œ

### 1. GeminiChatAdapter

```python
from src.infrastructure.ai_model.adapters.gemini_chat_adapter import GeminiChatAdapter
from src.domain.ai_model.entities.model_request import ModelRequest
from src.domain.ai_model.entities.model_config import ModelConfig

# ì–´ëŒ‘í„° ìƒì„±
adapter = GeminiChatAdapter(
    api_key="AIza...",
    model_name="gemini-2.0-flash-exp"
)

# ìš”ì²­ ìƒì„±
request = ModelRequest.create(
    messages=[
        {"role": "user", "content": "Hello, Gemini!"}
    ],
    config=ModelConfig(temperature=0.7, max_tokens=100)
)

# ì‹¤í–‰
result = adapter.execute(request)

if result.is_success():
    print(result.value.content)
```

### 2. GeminiWebSearchAdapter

```python
from src.infrastructure.ai_model.adapters.gemini_web_search_adapter import GeminiWebSearchAdapter

# ì›¹ ê²€ìƒ‰ ì–´ëŒ‘í„° ìƒì„±
adapter = GeminiWebSearchAdapter(
    api_key="AIza...",
    enable_grounding=True,
    dynamic_threshold=0.7
)

# ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸
request = ModelRequest.create(
    messages=[{
        "role": "user",
        "content": "What is the official Korean translation of 'Partido Popular'?"
    }],
    config=ModelConfig(temperature=0.3)
)

# ì‹¤í–‰ (ì›¹ ê²€ìƒ‰ ìë™ í™œì„±í™”)
result = adapter.execute(request)

if result.is_success():
    print(result.value.content)  # ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ì‘ë‹µ
```

---

## ğŸ“Š ì„±ëŠ¥ íŠ¹ì„±

### Gemini 2.0 Flash

| í•­ëª© | ê°’ |
|------|-----|
| **ì†ë„** | ë§¤ìš° ë¹ ë¦„ |
| **ì»¨í…ìŠ¤íŠ¸** | ìµœëŒ€ 2M í† í° |
| **ë¹„ìš©** | ì €ë ´ |
| **ì›¹ ê²€ìƒ‰** | Grounding ì§€ì› |
| **ë©€í‹°ëª¨ë‹¬** | í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ |

---

## ğŸ”œ ë‹¤ìŒ ë‹¨ê³„

1. âœ… Domain Layer êµ¬í˜„
2. ğŸ”œ ì›¹ ê°•í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±
3. ğŸ”œ Infrastructure Layer ì–´ëŒ‘í„° êµ¬í˜„
4. ğŸ”œ Application Layer ì„œë¹„ìŠ¤ êµ¬í˜„
5. ğŸ”œ E2E í…ŒìŠ¤íŠ¸

---

**ì™„ë£Œì¼**: 2025-10-13
**ë‹¤ìŒ ë¬¸ì„œ**: [02-single-shot-prompt.md](02-single-shot-prompt.md)
